# 预训练语言模型实现与应用

##案例简介

 2018年，Google提出了预训练语言模型BERT，该模型在各种NLP任务上都取得了很好的效果。与此同时，它的使用十分方便，可以快速地对于各种NLP任务进行适配。因此，BERT已经被广泛地使用到了各种NLP任务当中。在本案例中，我们会亲手将BERT适配到长文本关系抽取任务DocRED上，从中了解BERT的基本原理和技术细节。关系抽取是自然语言处理领域的重要任务，DocRED中大部分关系需要从多个句子中联合抽取，因此需要模型具备较强的获取和综合文章中信息的能力，尤其是抽取跨句关系的能力。

##BERT

BERT是目前最具代表性的预训练语言模型，如今预训练语言模型的新方法都是基于BERT进行改进的。研究者如今将各种预训练模型的使用代码整合到了`transformers`这个包当中，使得我们可以很方便快捷地使用各种各样的预训练语言模型。在本实验中，我们也将调用`transformers`来使用BERT完成文档级别关系抽取的任务。基于`transformers`的基础后，我们的主要工作就是将数据处理成BERT需要的输入格式，以及在BERT的基础上搭建一个能完成特定任务的模型。在本次实验中，我们的重点也将放在这两个方面。首先是对于数据的处理，对于给定的文本，我们需要使用BERT的tokenizer将文本切成subword，然后转换成对应的id输入进模型中。通常来说这个过程是比较简单的，但是针对于DocRED这个任务，我们需要有一些额外注意的事情。文档级关系抽取的目标是从一段话中确定两个实体之间的关系，为了让模型知道我们关心的两个实体是什么，我们需要在文本中插入四个额外的符号，将实体标注出来。与此同时，BERT模型是一个语言模型，为了能使其适配关系抽取任务，我们需要加入额外的神经网络，使得模型能够进行关系预测。通常来说这个神经网络就是将文本中的第一个字符拿出来输入到一个线性层中进行分类。

##数据和代码

本案例使用了DocRED的数据，并提供了一个简单的模型实现，包括数据的预处理、模型的训练、以及简单的评测。数据预处理的代码在gen_data.py里。在处理完数据之后，再运行train.py进行训练，训练流程的代码在config/Config.py里。注意由于预训练模型很大，因此需要调整batch size使得GPU能够放得下，于此同时为了提高batch size的绝对大小，可以使用[梯度累积](https://www.jiqizhixin.com/articles/2018-10-17-11)的技术。

以上代码是完整实现好的，我们需要同学对gen_data.py和config/Config.py中用# question标识的20余个问题进行回答，并运行模型，测试模型在有100% / 50% / 10% training data（通过随机sample原training set一部分的数据，10%代表低资源的设定）的情况下模型在dev set上的效果（如果服务器资源有限，也可以只测试10%的结果，并在报告中提及）

##评分要求

分数由两部分组成。首先，回答代码文件中标识的问题，并且训练模型，评测模型在开发集上的结果，这部分占80%，评分依据为模型的开发集性能和问题的回答情况。第二部分，进行进一步的探索和尝试，我们将在下一小节介绍可能的尝试。同学需要提交代码和报告，在报告中对于两部分的实验都进行介绍，主要包括开发集的结果以及尝试的具体内容。

##探索和尝试

完成对于测试数据的评测，并且提交到DocRED的[评测系统](https://competitions.codalab.org/competitions/20717)中。（推荐先完成该任务，主要考察同学将模型真正应用起来的能力）

使用别的预训练语言模型完成该实验，例如RoBERTa等。

对于模型进行改进，提升关系抽取的能力，这里可以参考一些DocRED最新工作，进行复现。

##参考资料

DocRED: A Large-Scale Document-Level Relation Extraction Dataset. ACL 2019.
